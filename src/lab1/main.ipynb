{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# \u041b\u0430\u0431\u043e\u0440\u0430\u0442\u043e\u0440\u043d\u0430\u044f \u0440\u0430\u0431\u043e\u0442\u0430 \u21161"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Loading extension for reloading editable packages (pip install -e .)\n%load_ext autoreload"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Reloading editable packages.\n%autoreload\n# from lab1.main import get_results"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0412\u0430\u0440\u0438\u0430\u043d\u0442 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u043d\u0438\u044f \u21163:\n1. \u041d\u043e\u043c\u0435\u0440 \u0433\u0440\u0443\u043f\u043f\u044b + 15 = 2 + 15 = 17\n2. \u041d\u043e\u043c\u0435\u0440 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 + 56 = 14 + 56 = 70\n3. \u0418\u04235 (\u041d\u043e\u043c\u0435\u0440 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 + 21) = 14 + 21 = 35"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import pickle\nfrom dataclasses import dataclass\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom numpy.typing import ArrayLike\nfrom PIL import Image\nfrom sklearn.metrics import classification_report\nfrom torch.utils.data import DataLoader, TensorDataset\n\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## \u0427\u0430\u0441\u0442\u044c 1. \u0417\u0430\u0434\u0430\u0447\u0430 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438 \u043f\u043e \u0442\u0435\u043e\u0440\u0435\u043c\u0435 \u0443\u043d\u0438\u0432\u0435\u0440\u0441\u0430\u043b\u044c\u043d\u043e\u0439 \u0430\u043f\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0430\u0446\u0438\u0438, \u0440\u0443\u0447\u043d\u043e\u0435 \u0434\u0438\u0444\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\n\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from operator import itemgetter  # noqa\n\nX = (np.arange(100) / 100 - 0.5).repeat(5)\n# \u041d\u0430\u0448\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043c\u044b \u043f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0432 \u0445\u043e\u0434\u0435 \u0430\u043f\u0440\u043e\u043a\u0441\u0438\u043c\u0430\u0446\u0438\u0438.\n# y = 1 / (1 + np.exp(-10 * X))  # \u0418\u0441\u0445\u043e\u0434\u043d\u0430\u044f.\ny = np.sin(X * 10) / 2\nyn = np.random.normal(scale=0.05, size=y.size) + y\n\nplt.plot(X, yn)\nplt.plot(X, y, linestyle=\"--\", c=\"k\")\n################################################\n\nHIDDEN_SIZE = 64\n\n\n# size = 1 for regression, size = number of classes for classification.\ndef np_to_tensor(arr: ArrayLike, size=1):\n    return torch.Tensor(arr.reshape(-1, size))\n\n\ntensor_X = np_to_tensor(X)\ntensor_y = np_to_tensor(yn)\n\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0451\u043c\ndef init_neural_network(hidden_size=HIDDEN_SIZE):\n    weights1 = (torch.rand(1, hidden_size) - 0.5) / 10\n    bias1 = torch.zeros(hidden_size)\n\n    weights2 = (torch.rand(hidden_size, 1) - 0.5) / 10\n    bias2 = torch.zeros(1)\n\n    return {\"weights1\": weights1, \"bias1\": bias1, \"weights2\": weights2, \"bias2\": bias2}\n\n\nweights1, bias1, weights2, bias2 = itemgetter(\"weights1\", \"bias1\", \"weights2\", \"bias2\")(\n    init_neural_network()\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0437\u0430\u0434\u0430\u0447\u0438 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043d\u0435\u043b\u0438\u043d\u0439\u043d\u043e\u0441\u0442\u0438\ndef relu(x: torch.Tensor):\n    return torch.maximum(x, torch.Tensor([0]))\n\n\n# \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434\ndef forward(x: torch.Tensor) -> torch.Tensor:\n    return (weights2.t() * relu((weights1 * x) + bias1)).sum(\n        axis=-1, keepdims=True\n    ) + bias2\n\n\ndef loss(y: torch.Tensor, y_: torch.Tensor) -> torch.Tensor:\n    return ((y - y_) ** 2).sum(axis=-1)\n\n\n# \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0440\u043e\u0445\u043e\u0434\ndef backward(X: torch.Tensor, y: torch.Tensor, y_pred: torch.Tensor):\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e y_pred\n    dL = 2 * (y_pred - y)\n    # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0441\u043a\u0440\u044b\u0442\u0433\u043e \u0441\u043b\u043e\u044f \u0434\u043e \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n    Ax = (weights1 * X) + bias1\n    # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0441\u043a\u0440\u044b\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u043f\u043e\u0441\u043b\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u0438\u044f \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n    A = relu(Ax)\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e weight_2\n    dW2 = torch.mm(A.t(), dL)\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e bias_2\n    db2 = dL.sum(axis=0)\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c \u0441\u043a\u0440\u044b\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u043f\u043e\u0441\u043b\u0435 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n    dA = torch.mm(dL, weights2.t())\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c \u0441\u043a\u0440\u044b\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u0434\u043e \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438\n    dA[Ax <= 0] = 0\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e weight_1\n    dW1 = torch.mm(X.t(), dA)\n    # \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e bias_1\n    db1 = dA.sum(axis=0)\n    # print(dW.shape, db.shape, dW2.shape, db2.shape)\n\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n\n\ndef optimize(params, grads, lr=0.001):\n    # \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a \u043f\u043e \u0432\u0441\u0435\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    W1, b1, W2, b2 = params\n    W1 -= lr * grads[\"dW1\"]\n    W2 -= lr * grads[\"dW2\"]\n    b1 -= lr * grads[\"db1\"]\n    b2 -= lr * grads[\"db2\"]\n\n    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n\n\n# 50 \u0442\u044b\u0441\u044f\u0447 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 == 50 \u0442\u044b\u0441\u044f\u0447 \u044d\u043f\u043e\u0445\nfor i in range(50000):\n    output = forward(tensor_X)\n    cur_loss = loss(output, tensor_y)\n    grads = backward(tensor_X, tensor_y, output)\n    params = [weights1, bias1, weights2, bias2]\n    optimized_params = optimize(params, grads, 1e-4)\n    weights1, bias1, weights2, bias2 = itemgetter(\"W1\", \"b1\", \"W2\", \"b2\")(\n        optimized_params\n    )\n\n    if (i + 1) % 10000 == 0:\n        plt.plot(X, output.numpy(), label=str(i + 1), alpha=0.5)\n\nplt.plot(X, y, linestyle=\"--\", c=\"k\", label=\"real\")\nplt.legend()\nplt.ylim(y.min(), y.max())\nprint(cur_loss.numpy().mean())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## \u0427\u0430\u0441\u0442\u044c 2. \u0411\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0430\u0432\u0442\u043e\u0434\u0438\u0444\u0444\u0438\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f PyTorch\n\u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def demonstrate_dataset(X: ArrayLike, y: ArrayLike):\n    plt.scatter(X[:, 0], X[:, 1], c=y)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# \u0414\u0430\u043d\u043d\u044b\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c:\nX_xor = np.random.randint(2, size=(1000, 2))\ny_xor = (X_xor[:, 0] + X_xor[:, 1]) % 2  # XOR\nX_xor = X_xor + np.random.normal(0, scale=0.1, size=X_xor.shape)\n\ndemonstrate_dataset(X_xor, y_xor)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.datasets import make_circles  # noqa\n\n# \u041a\u043e\u043b\u044c\u0446\u0430, \u0432\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435 \u0434\u0440\u0443\u0433 \u0432 \u0434\u0440\u0443\u0433\u0430.\nX_cirles, y_circles = make_circles(n_samples=1000, noise=0.025)\n\ndemonstrate_dataset(X_cirles, y_circles)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from sklearn.datasets import make_moons  # noqa\n\n# \u0412\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0435 \u0434\u0440\u0443\u0433 \u0432 \u0434\u0440\u0443\u0433\u0430 \u043c\u0435\u0441\u044f\u0446\u044b.\nX_moons, y_moons = make_moons(n_samples=1000, noise=0.025)\n\ndemonstrate_dataset(X_moons, y_moons)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "HIDDEN_SIZE = 48\n\n\n@dataclass()\nclass NeuralNetwork:\n    def __init__(self, X: ArrayLike, y: ArrayLike, hidden_size=HIDDEN_SIZE):\n        self.tensor_X = np_to_tensor(X, size=2)\n        self.tensor_y = np_to_tensor(y)\n\n        self.hidden_size = hidden_size\n        self.weights1, self.bias1, self.weights2, self.bias2 = itemgetter(\n            \"weights1\", \"bias1\", \"weights2\", \"bias2\"\n        )(self.init_neural_network(hidden_size=hidden_size))\n\n    # \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432 MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0451\u043c\n    def init_neural_network(self, hidden_size):\n        weights1 = (\n            ((torch.rand(2, hidden_size) - 0.5) / 10).detach().requires_grad_(True)\n        )\n        bias1 = torch.zeros(hidden_size, requires_grad=True)\n\n        weights2 = (\n            ((torch.rand(hidden_size, 1) - 0.5) / 10).detach().requires_grad_(True)\n        )\n        bias2 = torch.zeros(1, requires_grad=True)\n\n        return {\n            \"weights1\": weights1,\n            \"bias1\": bias1,\n            \"weights2\": weights2,\n            \"bias2\": bias2,\n        }\n\n    # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043d\u0435\u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0441\u0442\u0438\n    def sigmoid(self, x: torch.Tensor):\n        return 1 / (1 + torch.exp(-x))\n\n    # \u041f\u0440\u044f\u043c\u043e\u0439 \u043f\u0440\u043e\u0445\u043e\u0434\n    def forward(self, x: torch.Tensor):\n        hidden = torch.mm(x, self.weights1) + self.bias1\n        hidden_nonlin = self.sigmoid(hidden)\n        output = (self.weights2.t() * hidden_nonlin).sum(\n            axis=-1, keepdims=True\n        ) + self.bias2\n\n        return self.sigmoid(output)\n\n    # Logloss\n    def loss(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n        return (\n            -1\n            * (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred)).sum()\n        )\n\n    # lr - \u0448\u0430\u0433 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n    def model(self, learning_rate=1e-3, iterations=10_000):\n        params = [self.weights1, self.bias1, self.weights2, self.bias2]\n        losses = []\n        for _ in range(iterations):\n            output = self.forward(self.tensor_X)\n            lossval = self.loss(self.tensor_y, output)\n            lossval.backward()  # \u0442\u0443\u0442 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442\u0441\u044f \u0432 \u0440\u0430\u0431\u043e\u0442\u0443 autograd\n            for w in params:\n                with torch.no_grad():\n                    w -= w.grad * learning_rate  # \u043e\u0431\u043d\u043e\u0432\u043b\u044f\u0435\u043c \u0432\u0435\u0441\u0430\n                w.grad.zero_()  # \u0437\u0430\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u043d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u043b\u0438\u0441\u044c \u0437\u0430 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438\n            losses.append(lossval.item())\n\n        self.learning_results = {\"losses\": losses, \"output\": output}\n\n        return self.learning_results\n\n\nclassificationNN = NeuralNetwork(X_xor, y_xor)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 \u0437\u0430\u0434\u0430\u0447\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "learning_results = classificationNN.model()\n# \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0438\u0441\u0442\u043e\u0440\u0438\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u043f\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f\u043c\nplt.plot(learning_results[\"losses\"])"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def plot_classification_results(data: torch.Tensor, neural_model: NeuralNetwork):\n    x_coordinates = data[:, 0]\n    y_coordinates = data[:, 1]\n    x_coordinates_diff = x_coordinates.max() - x_coordinates.min()\n    y_coordinates_diff = y_coordinates.max() - y_coordinates.min()\n    left_boundary = x_coordinates.min() - 0.1 * x_coordinates_diff\n    right_boundary = x_coordinates.max() + 0.1 * x_coordinates_diff\n    bottom_boundary = y_coordinates.min() - 0.1 * y_coordinates_diff\n    top_boundary = y_coordinates.max() + 0.1 * y_coordinates_diff\n\n    grid = np.arange(left_boundary, right_boundary, 0.01)\n    grid_width = grid.size\n    surface = []\n    # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0442\u043e\u0447\u043a\u0438 \u043f\u043e \u0441\u0435\u0442\u043a\u0435\n    for x1 in grid:\n        for x2 in grid:\n            surface.append((x1, x2))\n    surface = np.array(surface)\n    print(surface.shape)\n    # \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0442\u043e\u0447\u0435\u043a \u043f\u043b\u043e\u0441\u043a\u043e\u0441\u0442\u0438, \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e \u0443\u0436\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u043c\n    # \u0432\u0435\u0441\u0430\u043c \u043f\u044b\u0442\u0430\u0435\u0442\u0441\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u043a\u0430\u043a\u043e\u043c\u0443 \u043a\u043b\u0430\u0441\u0441\u0443 \u043f\u0440\u0438\u043d\u0430\u0434\u043b\u0435\u0436\u0438\u0442 \u0442\u043e\u0447\u043a\u0430.\n    with torch.no_grad():\n        Z = neural_model.forward(torch.Tensor(surface)).detach().numpy()\n    # \u043c\u0435\u043d\u044f\u0435\u043c \u0444\u043e\u0440\u043c\u0443 \u0432 \u0432\u0438\u0434\u0435 \u0434\u0432\u0443\u0445\u043c\u0435\u0440\u043d\u043e\u0433\u043e \u043c\u0430\u0441\u0441\u0438\u0432\u0430\n    Z = Z.reshape(grid_width, grid_width)\n    xx = surface[:, 0].reshape(grid_width, grid_width)\n    yy = surface[:, 1].reshape(grid_width, grid_width)\n    # \u0440\u0438\u0441\u0443\u0435\u043c \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u044e\u0449\u0438\u0435 \u043f\u043e\u0432\u0435\u0440\u0445\u043d\u043e\u0441\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432\n    plt.contourf(xx, yy, Z, alpha=0.5)\n    # \u0440\u0438\u0441\u0443\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\n    plt.scatter(\n        x_coordinates,\n        y_coordinates,\n        c=neural_model.learning_results[\"output\"].detach().numpy() > 0.5,\n    )\n    # \u0437\u0430\u0434\u0430\u0451\u043c \u0433\u0440\u0430\u043d\u0438\u0446\u044b \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n    plt.xlim(left_boundary, right_boundary)\n    plt.ylim(bottom_boundary, top_boundary)\n\n\nplot_classification_results(X_xor, classificationNN)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "for hidden_size in [16, 32, 48, 64, 80]:\n    print(f\"{hidden_size = }\")\n    classificationNN = NeuralNetwork(X_cirles, y_circles, hidden_size)\n    classificationNN.model()\n    plt.figure()\n    plot_classification_results(X_cirles, classificationNN)\n    plt.show()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "for hidden_size in [16, 32, 48, 64, 80]:\n    print(f\"{hidden_size = }\")\n    classificationNN = NeuralNetwork(X_moons, y_moons, hidden_size)\n    classificationNN.model()\n    plt.figure()\n    plot_classification_results(X_moons, classificationNN)\n    plt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0412\u044b\u0432\u043e\u0434\n\u041d\u0430\u0448\u0430 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u0441 16 \u043d\u0435\u0439\u0440\u043e\u043d\u0430\u043c\u0438 \u0432 \u0441\u043a\u0440\u044b\u0442\u043e\u043c \u0441\u043b\u043e\u0435 \u0441\u043f\u0440\u0430\u0432\u0438\u043b\u0430\u0441\u044c \u0441 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0435\u0439 XOR.\n\u0414\u043b\u044f \u043a\u043e\u043b\u0435\u0446 \u043f\u043e\u043d\u0430\u0434\u043e\u0431\u0438\u043b\u043e\u0441\u044c \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0434\u043e 80. \u0414\u043b\u044f \u043b\u0443\u043d - \u0434\u043e 48."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## \u0427\u0430\u0441\u0442\u044c 3. \u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 CIFAR100\n### \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u0440\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u043a\u0430 \u043d\u0430\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 CIFAR100"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import os\nimport shutil\nimport urllib\nfrom pathlib import Path\n\nurl = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\nfilename = \"cifar-100-python.tar.gz\"\ndata_path = Path(\"data\")\n\ndata_path.mkdir(exist_ok=True)\n\nfile_path = data_path / filename\n\nif not os.path.isfile(file_path):\n    urllib.request.urlretrieve(url, file_path)\n    shutil.unpack_archive(file_path, extract_dir=data_path)\n    file_path.unlink()  # Remove archive after extracting it."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0427\u0442\u0435\u043d\u0438\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def stem_extensions(filename: Path):\n    extensions = \"\".join(filename.suffixes)\n\n    return str(filename).removesuffix(extensions)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "dataset_path = Path(stem_extensions(file_path))\n\nwith open(dataset_path / \"train\", \"rb\") as f:\n    data_train = pickle.load(f, encoding=\"latin1\")\nwith open(dataset_path / \"test\", \"rb\") as f:\n    data_test = pickle.load(f, encoding=\"latin1\")\n\n# \u041a\u043b\u0430\u0441\u0441\u044b \u043f\u043e \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0443.\nCLASSES = [17, 70, 35]\n\ntrain_X_raw = data_train[\"data\"].reshape(-1, 3, 32, 32)\ntrain_X_raw = np.transpose(train_X_raw, [0, 2, 3, 1])  # NCHW -> NHWC\ntrain_y_raw = np.array(data_train[\"fine_labels\"])\nmask = np.isin(train_y_raw, CLASSES)\ntrain_X = train_X_raw[mask].copy()\ntrain_y = train_y_raw[mask].copy()\ntrain_y = np.unique(train_y, return_inverse=1)[1]\ndel data_train\n\ntest_X = data_test[\"data\"].reshape(-1, 3, 32, 32)\ntest_X = np.transpose(test_X, [0, 2, 3, 1])\ntest_y = np.array(data_test[\"fine_labels\"])\nmask = np.isin(test_y, CLASSES)\ntest_X = test_X[mask].copy()\ntest_y = test_y[mask].copy()\ntest_y = np.unique(test_y, return_inverse=1)[1]\ndel data_test\n\n# print(train_y_raw.tolist())"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def createImage(data: ArrayLike):\n    return Image.fromarray(data).resize((256, 256))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# Source: https://stackoverflow.com/a/47334314\ndef grid_display(list_of_images, list_of_titles=[], no_of_columns=2, figsize=(10, 10)):\n    fig = plt.figure(figsize=figsize)\n    column = 0\n    for i in range(len(list_of_images)):\n        column += 1\n        #  check for end of column and create a new figure\n        if column == no_of_columns + 1:\n            fig = plt.figure(figsize=figsize)\n            column = 1\n        fig.add_subplot(1, no_of_columns, column)\n        plt.imshow(list_of_images[i])\n        plt.axis(\"off\")\n        if len(list_of_titles) >= len(list_of_images):\n            plt.title(list_of_titles[i])"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# \u041f\u043e 3 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043a\u043b\u0430\u0441\u0441\u0430 \u0438\u0437 \u0432\u044b\u0431\u043e\u0440\u043a\u0438.\nnumber_of_images_per_class_to_show = 3\n\nfor class_id in CLASSES:\n    print(f\"{class_id = }:\")\n    i = number_of_images_per_class_to_show\n    image_index_for_class = -1\n    class_images = []\n    image_indices = []\n\n    while i > 0:\n        image_index_for_class = train_y_raw.tolist().index(\n            class_id, image_index_for_class + 1\n        )\n        image_indices.append(image_index_for_class)\n        class_images.append(createImage(train_X_raw[image_index_for_class]))\n        i -= 1\n    grid_display(class_images, image_indices, number_of_images_per_class_to_show)\n    plt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 Pytorch DataLoader'a"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def create_dataloader(batch_size=128):\n    dataloader: dict[str, DataLoader] = {}\n    for (X, y), part in zip([(train_X, train_y), (test_X, test_y)], [\"train\", \"test\"]):\n        tensor_x = torch.Tensor(X)\n        tensor_y = (\n            F.one_hot(torch.Tensor(y).to(torch.int64), num_classes=len(CLASSES)) / 1.0\n        )\n        dataset = TensorDataset(tensor_x, tensor_y)  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n        dataloader[part] = DataLoader(\n            dataset, batch_size=batch_size, shuffle=True\n        )  # \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u044d\u043a\u0437\u0435\u043c\u043f\u043b\u044f\u0440\u0430 \u043a\u043b\u0430\u0441\u0441\u0430 DataLoader\n\n    return dataloader"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 Pytorch \u043c\u043e\u0434\u0435\u043b\u0438 \u043c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0446\u0435\u043f\u0442\u0440\u043e\u043d\u0430 \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0435\u043c"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class Normalize(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalize, self).__init__()\n        self.mean = torch.tensor(mean)\n        self.std = torch.tensor(std)\n\n    def forward(self, input):\n        x = input / 255.0\n        x = x - self.mean\n        x = x / self.std\n\n        return torch.flatten(x, start_dim=1)  # nhwc -> nm\n\n\nclass Cifar100_MLP(nn.Module):\n    def __init__(self, hidden_size=32, classes=100):\n        super(Cifar100_MLP, self).__init__()\n        # https://blog.jovian.ai/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1\n        self.norm = Normalize([0.5074, 0.4867, 0.4411], [0.2011, 0.1987, 0.2025])\n        self.seq = nn.Sequential(\n            nn.Linear(32 * 32 * 3, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, classes),\n        )\n\n    def forward(self, input):\n        x = self.norm(input)\n\n        return self.seq(x)\n\n\nHIDDEN_SIZE = 10\nmodel = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\nmodel"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u044d\u043f\u043e\u0445\u0430\u043c"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "EPOCHS = 250\n\n\ndef train(\n    model: nn.Module,\n    criterion: nn.CrossEntropyLoss,\n    optimizer: optim.Optimizer,\n    dataloader: dict[str, DataLoader],\n    epochs=EPOCHS,\n):\n    steps_per_epoch = len(dataloader[\"train\"])\n    steps_per_epoch_val = len(dataloader[\"test\"])\n\n    for epoch in range(epochs):  # \u043f\u0440\u043e\u0445\u043e\u0434 \u043f\u043e \u043d\u0430\u0431\u043e\u0440\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\n        running_loss = 0.0\n        model.train()\n        for i, batch in enumerate(dataloader[\"train\"], 0):\n            # \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0434\u043d\u043e\u0433\u043e \u043c\u0438\u043d\u0438\u0431\u0430\u0442\u0447\u0430; batch \u044d\u0442\u043e \u0434\u0432\u0443\u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 [inputs, labels]\n            inputs, labels = batch\n\n            # \u043e\u0447\u0438\u0449\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u0448\u043b\u044b\u0445 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u0441 \u043f\u0440\u043e\u0448\u043b\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438\n            optimizer.zero_grad()\n\n            # \u043f\u0440\u044f\u043c\u043e\u0439 + \u043e\u0431\u0440\u0430\u0442\u043d\u044b\u0439 \u043f\u0440\u043e\u0445\u043e\u0434\u044b + \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u044f\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            # loss = F.cross_entropy(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # \u0434\u043b\u044f \u043f\u043e\u0434\u0441\u0447\u0451\u0442\u0430 \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\n            running_loss += loss.item()\n        print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / steps_per_epoch:.3f}\")\n        running_loss = 0.0\n        model.eval()\n        with torch.no_grad():  # \u043e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0434\u0438\u0444\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n            for i, data in enumerate(dataloader[\"test\"], 0):\n                inputs, labels = data\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                running_loss += loss.item()\n        print(\n            f\"[{epoch + 1}, {i + 1:5d}] val loss: {running_loss / steps_per_epoch_val:.3f}\"\n        )\n    print(\"\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0437\u0430\u043a\u043e\u043d\u0447\u0435\u043d\u043e\")\n\n    return dataloader"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0412\u044b\u0431\u043e\u0440 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043f\u043e\u0442\u0435\u0440\u044c \u0438 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def train_classifier(\n    model: nn.Module, learning_rate=0.005, batch_size=128, epochs=EPOCHS\n):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    dataloader = create_dataloader(batch_size=batch_size)\n\n    return train(\n        model,\n        criterion=criterion,\n        optimizer=optimizer,\n        dataloader=dataloader,\n        epochs=epochs,\n    )\n\n\ndataloader = train_classifier(model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043a\u043b\u0430\u0441\u0441\u0430\u043c \u043d\u0430 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def report_classification_results(dataloader: DataLoader):\n    y_pred = []\n    y_true = []\n    with torch.no_grad():  # \u043e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0434\u0438\u0444\u0444\u0435\u0440\u0435\u043d\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f\n        for _, data in enumerate(dataloader, 0):\n            inputs, labels = data\n\n            outputs = model(inputs).detach().numpy()\n            y_pred.append(outputs)\n            y_true.append(labels.numpy())\n        y_true = np.concatenate(y_true)\n        y_pred = np.concatenate(y_pred)\n        print(\n            classification_report(\n                y_true.argmax(axis=-1),\n                y_pred.argmax(axis=-1),\n                digits=4,\n                target_names=list(map(str, CLASSES)),\n            )\n        )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "def compare_classification_reports(dataloader: dict[str, DataLoader]):\n    for part in [\"train\", \"test\"]:\n        print(part)\n        report_classification_results(dataloader[part])\n        part != \"test\" and print(\"-\" * 53)\n\n\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0410\u043d\u0430\u043b\u0438\u0437 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n\u041a\u0430\u043a \u0432\u0438\u0434\u043d\u043e, \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u0431\u044b\u043b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d \u043a\u043b\u0430\u0441\u0441 \u0441 \u0438\u0434\u0435\u043d\u0442\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u043e\u043c 17, \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439\n\u0441\u043e\u0431\u043e\u0439 \u0437\u0430\u043c\u043a\u0438. \u0412\u0435\u0440\u043e\u044f\u0442\u043d\u0435\u0435 \u0432\u0441\u0435\u0433\u043e \u044d\u0442\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\u043c \u043e\u0431\u0449\u0438\u0445 \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0447\u0435\u0440\u0442 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432\n\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u0445: \u0443 \u0437\u0430\u043c\u043a\u043e\u0432 \u0438\u0445 \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u043c\u0435\u043d\u044c\u0448\u0435 \u043f\u043e \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044e \u0441, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, \u0440\u0435\u0431\u0451\u043d\u043a\u043e\u043c,\n\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u0440\u0430\u0437\u043d\u043e\u0433\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430, \u0440\u0430\u0441\u044b \u0438 \u043c\u043e\u0436\u0435\u0442 \u0431\u044b\u0442\u044c \u043e\u0434\u0435\u0442 \u043f\u043e-\u0440\u0430\u0437\u043d\u043e\u043c\u0443."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u041d\u0430 \u043b\u0438\u0446\u043e \u0442\u0430\u043a \u0436\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435: \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0431\u044b\u043b\u0438 \u043f\u043e\u0447\u0442\u0438 \u0438\u0434\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438,\n\u0430 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0441\u0440\u0435\u0434\u043d\u0438\u043c\u0438. \u0421\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0443\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u044f\n\u044d\u0442\u043e\u0433\u043e \u0444\u0435\u043d\u043e\u043c\u0435\u043d\u0430 \u0432 \u043d\u0430\u0434\u0435\u0436\u0434\u0435 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u043c\u043e\u0434\u0435\u043b\u0438."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0423\u043c\u0435\u043d\u044c\u0448\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\ndataloader = train_classifier(model, epochs=51)\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0432\u0435\u0441\u043e\u0432"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "weights = list(model.parameters())[0].detach().numpy()\nprint(weights.shape)\nfig, ax = plt.subplots(1, weights.shape[0], figsize=(3 * weights.shape[0], 3))\nfor i, \u03c9 in enumerate(weights):\n    \u03c9 = \u03c9.reshape(32, 32, 3)\n    \u03c9 -= np.percentile(\u03c9, 1, axis=[0, 1])\n    \u03c9 /= np.percentile(\u03c9, 99, axis=[0, 1])\n    \u03c9 = np.clip(\u03c9, 0, 1)\n    ax[i].imshow(\u03c9)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u041f\u043e \u043b\u043e\u0433\u0430\u043c \u043f\u043e\u0442\u0435\u0440\u044c \u0431\u044b\u043b\u043e \u0432\u044b\u044f\u0441\u043d\u0435\u043d\u043e, \u0447\u0442\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438\n\u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f \u043d\u0430 52 \u044d\u043f\u043e\u0445\u0430\u0445, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0441\u0442\u0430\u0432\u0438\u043c 51."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0418\u0437\u043c\u0435\u043d\u0438\u043c batch_size, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044f \u043e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439. \u0414\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e\n\u044d\u043f\u043e\u0445 \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043c \u0432 \u0442\u043e \u0436\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0437, \u0432\u043e \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043b\u0438 batch_size."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\ndataloader = train_classifier(model, epochs=25, batch_size=256)\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u041e\u0431\u0449\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u043c\u0435\u043d\u044f\u043b\u0438\u0441\u044c, \u043e\u0434\u043d\u0430\u043a\u043e\n\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u043a \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435 \u043e\u0447\u0435\u043d\u044c \u0431\u043b\u0438\u0437\u043a\u043e \u043a 1. \u041c\u043e\u0436\u043d\u043e\n\u0441 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e \u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u0447\u0442\u043e \u043d\u0430 \u0434\u0430\u043d\u043d\u043e\u043c \u044d\u0442\u0430\u043f\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0435\u0442\u0441\u044f."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u041f\u043e\u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u0435\u0449\u0451 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c:\n\u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043c \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u043e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0414\u043b\u044f \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u043d\u043e\u0439 \u0432 \u0434\u0432\u0430 \u0440\u0430\u0437\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043d\u0430\u0447\u0430\u043b\u043e\u0441\u044c \u0432 \u0440\u0430\u0439\u043e\u043d\u0435\n78-79 \u044d\u043f\u043e\u0445. \u041f\u043e\u0441\u0442\u0430\u0432\u0438\u0432 77 \u044d\u043f\u043e\u0445\u0438 \u043c\u044b \u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = Cifar100_MLP(hidden_size=HIDDEN_SIZE, classes=len(CLASSES))\ndataloader = train_classifier(model, learning_rate=0.0025, epochs=77, batch_size=256)\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u041f\u043e\u043c\u0435\u043d\u044f\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0435\u0439\u0440\u043e\u043d\u043e\u0432 \u0432 \u0441\u043a\u0440\u044b\u0442\u043e\u043c \u0441\u043b\u043e\u0435. \u0422\u0430\u043a \u043a\u0430\u043a \u043c\u043e\u0434\u0435\u043b\u044c \u0438\u0437-\u0437\u0430 \u044d\u0442\u043e\u0433\u043e\n\u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u043e\u043c\u0435\u043d\u044f\u0435\u043c, \u043f\u043e\u0434\u0441\u0442\u0440\u043e\u0438\u043c \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0443\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u044f\n\u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u043e\u0441\u0442\u0430\u0440\u0430\u0435\u043c\u0441\u044f \u043d\u0430\u0439\u0442\u0438 \u043c\u0430\u043a\u0441\u0438\u043c\u0443\u043c, \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u043c\u043e\u0436\u0435\u0442 \u0434\u043e\u0441\u0442\u0438\u0433\u0438\u0447\u044c \u043c\u043e\u0434\u0435\u043b\u044c."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = Cifar100_MLP(hidden_size=2 * HIDDEN_SIZE, classes=len(CLASSES))\ndataloader = train_classifier(model, learning_rate=0.0025, epochs=88, batch_size=256)\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0435\u0449\u0451 \u043e\u0434\u0438\u043d \u0441\u043a\u0440\u044b\u0442\u044b\u0439 \u0441\u043b\u043e\u0439."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "class Cifar100_MLP_2(nn.Module):\n    def __init__(self, hidden_sizes=[32, 26], classes=100):\n        super(Cifar100_MLP_2, self).__init__()\n        # https://blog.jovian.ai/image-classification-of-cifar100-dataset-using-pytorch-8b7145242df1\n        self.norm = Normalize([0.5074, 0.4867, 0.4411], [0.2011, 0.1987, 0.2025])\n        self.seq = nn.Sequential(\n            nn.Linear(32 * 32 * 3, hidden_sizes[0]),\n            nn.ReLU(),\n            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n            nn.ReLU(),\n            nn.Linear(hidden_sizes[1], classes),\n        )\n\n    def forward(self, input):\n        x = self.norm(input)\n\n        return self.seq(x)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model = Cifar100_MLP_2(classes=len(CLASSES))\ndataloader = train_classifier(model, learning_rate=0.0025, epochs=233, batch_size=256)\ncompare_classification_reports(dataloader)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\nonnx_model_filename = \"cifar100_cnn.onnx\"\nx = torch.randn(1, 32, 32, 3, requires_grad=True).to(device)\ntorch_out = model(x)\n\n# \u044d\u043a\u0441\u043f\u043e\u0440\u0442 \u043c\u043e\u0434\u0435\u043b\u0438\ntorch.onnx.export(\n    model,  # \u043c\u043e\u0434\u0435\u043b\u044c\n    x,  # \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u0442\u0435\u043d\u0437\u043e\u0440 (\u0438\u043b\u0438 \u043a\u043e\u0440\u0442\u0435\u0436 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432)\n    model_path/onnx_model_filename,  # \u043a\u0443\u0434\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c (\u043b\u0438\u0431\u043e \u043f\u0443\u0442\u044c \u043a \u0444\u0430\u0439\u043b\u0443 \u043b\u0438\u0431\u043e fileObject)\n    export_params=True,  # \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u0442 \u0432\u0435\u0441\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0432\u043d\u0443\u0442\u0440\u0438 \u0444\u0430\u0439\u043b\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n    opset_version=9,  # \u0432\u0435\u0440\u0441\u0438\u044f ONNX\n    do_constant_folding=True,  # \u0441\u043b\u0435\u0434\u0443\u0435\u0442 \u043b\u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0442\u044c \u0443\u043a\u043e\u0440\u0430\u0447\u0438\u0432\u0430\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442 \u0434\u043b\u044f \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0446\u0438\u0438\n    input_names=[\"input\"],  # \u0438\u043c\u044f \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f\n    output_names=[\"output\"],  # \u0438\u043c\u044f \u0432\u044b\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f\n    dynamic_axes={\n        \"input\": {\n            0: \"batch_size\"\n        },  # \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u043d\u044b\u0435 \u043e\u0441\u0438, \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u0430\u043a\u0435\u0442\u0430\n        \"output\": {0: \"batch_size\"},\n    },\n)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}